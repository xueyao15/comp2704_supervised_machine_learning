{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DftYr</th>\n",
       "      <th>GP</th>\n",
       "      <th>G</th>\n",
       "      <th>A</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>PTS</th>\n",
       "      <th>Shifts</th>\n",
       "      <th>TOI</th>\n",
       "      <th>TOI/GP</th>\n",
       "      <th>...</th>\n",
       "      <th>TKA</th>\n",
       "      <th>PENT</th>\n",
       "      <th>PEND</th>\n",
       "      <th>OPS</th>\n",
       "      <th>DPS</th>\n",
       "      <th>PS</th>\n",
       "      <th>OTOI</th>\n",
       "      <th>GS</th>\n",
       "      <th>GS/G</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.665158</td>\n",
       "      <td>0.631581</td>\n",
       "      <td>0.562553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472622</td>\n",
       "      <td>0.417808</td>\n",
       "      <td>0.547445</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.912349</td>\n",
       "      <td>0.542995</td>\n",
       "      <td>0.659459</td>\n",
       "      <td>0.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.950617</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.721719</td>\n",
       "      <td>0.631836</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386167</td>\n",
       "      <td>0.568493</td>\n",
       "      <td>0.503650</td>\n",
       "      <td>0.078261</td>\n",
       "      <td>0.364865</td>\n",
       "      <td>0.175758</td>\n",
       "      <td>0.831395</td>\n",
       "      <td>0.059903</td>\n",
       "      <td>0.329730</td>\n",
       "      <td>0.260870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.890271</td>\n",
       "      <td>0.815014</td>\n",
       "      <td>0.785106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587896</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.540146</td>\n",
       "      <td>0.104348</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.321212</td>\n",
       "      <td>0.799686</td>\n",
       "      <td>0.205797</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.901235</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.773379</td>\n",
       "      <td>0.725383</td>\n",
       "      <td>0.760851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541787</td>\n",
       "      <td>0.643836</td>\n",
       "      <td>0.503650</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.503030</td>\n",
       "      <td>0.757226</td>\n",
       "      <td>0.399034</td>\n",
       "      <td>0.589189</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.432099</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.285445</td>\n",
       "      <td>0.271271</td>\n",
       "      <td>0.548936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170029</td>\n",
       "      <td>0.335616</td>\n",
       "      <td>0.197080</td>\n",
       "      <td>0.165217</td>\n",
       "      <td>0.094595</td>\n",
       "      <td>0.115152</td>\n",
       "      <td>0.402721</td>\n",
       "      <td>0.110145</td>\n",
       "      <td>0.427027</td>\n",
       "      <td>0.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.301587</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.337209</td>\n",
       "      <td>0.508296</td>\n",
       "      <td>0.433026</td>\n",
       "      <td>0.474468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317003</td>\n",
       "      <td>0.417808</td>\n",
       "      <td>0.467153</td>\n",
       "      <td>0.286957</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.745332</td>\n",
       "      <td>0.473430</td>\n",
       "      <td>0.697297</td>\n",
       "      <td>0.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.876543</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.709302</td>\n",
       "      <td>0.599548</td>\n",
       "      <td>0.597429</td>\n",
       "      <td>0.618298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452450</td>\n",
       "      <td>0.561644</td>\n",
       "      <td>0.788321</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.202703</td>\n",
       "      <td>0.442424</td>\n",
       "      <td>0.787276</td>\n",
       "      <td>0.640580</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.765460</td>\n",
       "      <td>0.686572</td>\n",
       "      <td>0.656170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414986</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.649635</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.206061</td>\n",
       "      <td>0.869354</td>\n",
       "      <td>0.283092</td>\n",
       "      <td>0.491892</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.92</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.492063</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.690045</td>\n",
       "      <td>0.646906</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515850</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.678832</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.955327</td>\n",
       "      <td>0.591304</td>\n",
       "      <td>0.691892</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.305532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.040580</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>622 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DftYr        GP      G         A        A1        A2       PTS    Shifts  \\\n",
       "416   0.64  1.000000  0.575  0.428571  0.500000  0.258065  0.581395  0.665158   \n",
       "618   0.72  0.950617  0.025  0.111111  0.052632  0.161290  0.093023  0.721719   \n",
       "63    0.48  0.987654  0.025  0.222222  0.184211  0.225806  0.174419  0.890271   \n",
       "611   0.76  0.901235  0.150  0.269841  0.210526  0.290323  0.267442  0.773379   \n",
       "391   0.80  0.432099  0.150  0.079365  0.052632  0.096774  0.127907  0.285445   \n",
       "..     ...       ...    ...       ...       ...       ...       ...       ...   \n",
       "218   0.80  0.777778  0.250  0.301587  0.289474  0.258065  0.337209  0.508296   \n",
       "223   0.84  0.876543  0.450  0.682540  0.789474  0.419355  0.709302  0.599548   \n",
       "271   0.44  0.962963  0.025  0.238095  0.184211  0.258065  0.186047  0.765460   \n",
       "474   0.92  1.000000  0.675  0.492063  0.394737  0.516129  0.674419  0.690045   \n",
       "355   0.88  0.000000  0.000  0.000000  0.000000  0.000000  0.000000  0.004525   \n",
       "\n",
       "          TOI    TOI/GP  ...       TKA      PENT      PEND       OPS  \\\n",
       "416  0.631581  0.562553  ...  0.472622  0.417808  0.547445  0.521739   \n",
       "618  0.631836  0.600000  ...  0.386167  0.568493  0.503650  0.078261   \n",
       "63   0.815014  0.785106  ...  0.587896  0.726027  0.540146  0.104348   \n",
       "611  0.725383  0.760851  ...  0.541787  0.643836  0.503650  0.260870   \n",
       "391  0.271271  0.548936  ...  0.170029  0.335616  0.197080  0.165217   \n",
       "..        ...       ...  ...       ...       ...       ...       ...   \n",
       "218  0.433026  0.474468  ...  0.317003  0.417808  0.467153  0.286957   \n",
       "223  0.597429  0.618298  ...  0.452450  0.561644  0.788321  0.565217   \n",
       "271  0.686572  0.656170  ...  0.414986  0.616438  0.649635  0.147826   \n",
       "474  0.646906  0.580000  ...  0.515850  0.534247  0.678832  0.600000   \n",
       "355  0.003233  0.305532  ...  0.000000  0.006849  0.007299  0.147826   \n",
       "\n",
       "          DPS        PS      OTOI        GS      GS/G       Age  \n",
       "416  0.216216  0.418182  0.912349  0.542995  0.659459  0.304348  \n",
       "618  0.364865  0.175758  0.831395  0.059903  0.329730  0.260870  \n",
       "63   0.648649  0.321212  0.799686  0.205797  0.432432  0.521739  \n",
       "611  0.810811  0.503030  0.757226  0.399034  0.589189  0.217391  \n",
       "391  0.094595  0.115152  0.402721  0.110145  0.427027  0.130435  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "218  0.216216  0.254545  0.745332  0.473430  0.697297  0.130435  \n",
       "223  0.202703  0.442424  0.787276  0.640580  0.783784  0.086957  \n",
       "271  0.324324  0.206061  0.869354  0.283092  0.491892  0.565217  \n",
       "474  0.256757  0.490909  0.955327  0.591304  0.691892  0.043478  \n",
       "355  0.040541  0.072727  0.001041  0.040580  0.270270  0.043478  \n",
       "\n",
       "[622 rows x 70 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the features training set\n",
    "with open('subsets_ft.pkl', 'rb') as file:\n",
    "    features_training= pickle.load(file)\n",
    "features_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DftYr</th>\n",
       "      <th>GP</th>\n",
       "      <th>G</th>\n",
       "      <th>A</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>PTS</th>\n",
       "      <th>Shifts</th>\n",
       "      <th>TOI</th>\n",
       "      <th>TOI/GP</th>\n",
       "      <th>...</th>\n",
       "      <th>TKA</th>\n",
       "      <th>PENT</th>\n",
       "      <th>PEND</th>\n",
       "      <th>OPS</th>\n",
       "      <th>DPS</th>\n",
       "      <th>PS</th>\n",
       "      <th>OTOI</th>\n",
       "      <th>GS</th>\n",
       "      <th>GS/G</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.059578</td>\n",
       "      <td>0.047122</td>\n",
       "      <td>0.294468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011527</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.182609</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.114977</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.454054</td>\n",
       "      <td>0.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.092766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.042512</td>\n",
       "      <td>0.372973</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.773379</td>\n",
       "      <td>0.655592</td>\n",
       "      <td>0.599149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452450</td>\n",
       "      <td>0.472603</td>\n",
       "      <td>0.452555</td>\n",
       "      <td>0.495652</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.964684</td>\n",
       "      <td>0.486957</td>\n",
       "      <td>0.627027</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.320988</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.151584</td>\n",
       "      <td>0.109517</td>\n",
       "      <td>0.222128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.260274</td>\n",
       "      <td>0.167883</td>\n",
       "      <td>0.139130</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.331531</td>\n",
       "      <td>0.051208</td>\n",
       "      <td>0.340541</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>0.84</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.460317</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>0.700603</td>\n",
       "      <td>0.636480</td>\n",
       "      <td>0.568085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688761</td>\n",
       "      <td>0.397260</td>\n",
       "      <td>0.525547</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>0.940119</td>\n",
       "      <td>0.477295</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.604938</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.190045</td>\n",
       "      <td>0.150053</td>\n",
       "      <td>0.119149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121037</td>\n",
       "      <td>0.116438</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.113043</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.588957</td>\n",
       "      <td>0.079227</td>\n",
       "      <td>0.362162</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.150452</td>\n",
       "      <td>0.127009</td>\n",
       "      <td>0.385106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077810</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>0.156522</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.236576</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.383784</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.679012</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.535068</td>\n",
       "      <td>0.504497</td>\n",
       "      <td>0.686383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351585</td>\n",
       "      <td>0.404110</td>\n",
       "      <td>0.328467</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.157576</td>\n",
       "      <td>0.582710</td>\n",
       "      <td>0.142029</td>\n",
       "      <td>0.421622</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.322398</td>\n",
       "      <td>0.272974</td>\n",
       "      <td>0.362553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233429</td>\n",
       "      <td>0.294521</td>\n",
       "      <td>0.255474</td>\n",
       "      <td>0.191304</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.157576</td>\n",
       "      <td>0.559750</td>\n",
       "      <td>0.172947</td>\n",
       "      <td>0.470270</td>\n",
       "      <td>0.391304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.719080</td>\n",
       "      <td>0.826424</td>\n",
       "      <td>0.798723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636888</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.664234</td>\n",
       "      <td>0.504348</td>\n",
       "      <td>0.418919</td>\n",
       "      <td>0.496970</td>\n",
       "      <td>0.830905</td>\n",
       "      <td>0.425121</td>\n",
       "      <td>0.583784</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     DftYr        GP      G         A        A1        A2       PTS    Shifts  \\\n",
       "9     0.84  0.111111  0.075  0.000000  0.000000  0.000000  0.034884  0.059578   \n",
       "511   0.72  0.000000  0.000  0.000000  0.000000  0.000000  0.000000  0.001131   \n",
       "308   0.92  0.987654  0.500  0.507937  0.578947  0.322581  0.604651  0.773379   \n",
       "219   0.72  0.320988  0.050  0.015873  0.000000  0.032258  0.034884  0.151584   \n",
       "578   0.84  1.000000  0.400  0.460317  0.289474  0.580645  0.523256  0.700603   \n",
       "..     ...       ...    ...       ...       ...       ...       ...       ...   \n",
       "720   0.44  0.604938  0.050  0.015873  0.000000  0.032258  0.034884  0.190045   \n",
       "281   0.84  0.259259  0.025  0.031746  0.000000  0.064516  0.034884  0.150452   \n",
       "53    0.72  0.679012  0.050  0.142857  0.105263  0.161290  0.127907  0.535068   \n",
       "147   0.72  0.592593  0.150  0.126984  0.078947  0.161290  0.162791  0.322398   \n",
       "395   0.76  0.987654  0.275  0.555556  0.447368  0.580645  0.534884  0.719080   \n",
       "\n",
       "          TOI    TOI/GP  ...       TKA      PENT      PEND       OPS  \\\n",
       "9    0.047122  0.294468  ...  0.011527  0.020548  0.021898  0.182609   \n",
       "511  0.000983  0.092766  ...  0.000000  0.000000  0.000000  0.147826   \n",
       "308  0.655592  0.599149  ...  0.452450  0.472603  0.452555  0.495652   \n",
       "219  0.109517  0.222128  ...  0.031700  0.260274  0.167883  0.139130   \n",
       "578  0.636480  0.568085  ...  0.688761  0.397260  0.525547  0.382609   \n",
       "..        ...       ...  ...       ...       ...       ...       ...   \n",
       "720  0.150053  0.119149  ...  0.121037  0.116438  0.138686  0.113043   \n",
       "281  0.127009  0.385106  ...  0.077810  0.095890  0.065693  0.156522   \n",
       "53   0.504497  0.686383  ...  0.351585  0.404110  0.328467  0.147826   \n",
       "147  0.272974  0.362553  ...  0.233429  0.294521  0.255474  0.191304   \n",
       "395  0.826424  0.798723  ...  0.636888  0.520548  0.664234  0.504348   \n",
       "\n",
       "          DPS        PS      OTOI        GS      GS/G       Age  \n",
       "9    0.054054  0.109091  0.114977  0.065700  0.454054  0.130435  \n",
       "511  0.027027  0.072727  0.001517  0.042512  0.372973  0.043478  \n",
       "308  0.256757  0.418182  0.964684  0.486957  0.627027  0.000000  \n",
       "219  0.054054  0.072727  0.331531  0.051208  0.340541  0.434783  \n",
       "578  0.216216  0.327273  0.940119  0.477295  0.621622  0.086957  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "720  0.040541  0.054545  0.588957  0.079227  0.362162  0.521739  \n",
       "281  0.135135  0.127273  0.236576  0.066667  0.383784  0.086957  \n",
       "53   0.216216  0.157576  0.582710  0.142029  0.421622  0.217391  \n",
       "147  0.148649  0.157576  0.559750  0.172947  0.470270  0.391304  \n",
       "395  0.418919  0.496970  0.830905  0.425121  0.583784  0.173913  \n",
       "\n",
       "[78 rows x 70 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the features validation set\n",
    "with open('subsets_fv.pkl', 'rb') as file:\n",
    "    features_validation= pickle.load(file)\n",
    "features_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456    8000000.0\n",
       "674    1000000.0\n",
       "69     5600000.0\n",
       "667    3500000.0\n",
       "431     950000.0\n",
       "         ...    \n",
       "243    4500000.0\n",
       "248    6750000.0\n",
       "298    4500000.0\n",
       "519    6500000.0\n",
       "390     925000.0\n",
       "Name: Salary, Length: 622, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Load the label training set\n",
    "with open('subsets_lt.pkl', 'rb') as file:\n",
    "    label_training= pickle.load(file)\n",
    "label_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11      667500.0\n",
       "558     630000.0\n",
       "337     832500.0\n",
       "244    2000000.0\n",
       "634    4000000.0\n",
       "         ...    \n",
       "784     750000.0\n",
       "309     632500.0\n",
       "59     5250000.0\n",
       "161    3500000.0\n",
       "435    4500000.0\n",
       "Name: Salary, Length: 78, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the label validation set\n",
    "with open('subsets_lv.pkl', 'rb') as file:\n",
    "    label_validation= pickle.load(file)\n",
    "label_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with at least three different values of the C parameter. Explain what this parameter controls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, a list has been created to test different C values (1, 10, 100, 1000, 2000) in the grid search.\n",
    "\n",
    "C parameter controls the decision boundary between trying to classify the points correctly, meaning penalizing more for misclassifications and trying to space out the lines, allowing more data points to be misclassified. \n",
    "\n",
    "If C is large, then the error formula is dominated by the classification error, so our classifier focuses more on classifying the data points correctly. \n",
    "\n",
    "If C is small, then the formula is dominated by the distance error, so our classifier focuses more on keeping the lines far apart.\n",
    "\n",
    "In our case, C=2000 works the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the linear, rbf (with different choices of gamma) and polynomial kernels. Comment on which of these works best for your data and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tried three kernals in our grid search. **Polynomial kernel** works the best.\n",
    "\n",
    "**Linear**\n",
    "\n",
    "The linear kernal is for dataset that demonstrates a linear realtionship between the feature and label. Since our data is nonlinear, it may not capture the complex nonlinear patterns in our dataset.\n",
    "\n",
    "**RBF**\n",
    "\n",
    "We’ve tried the radial basis function kernel with a different choices of gamma (1,10,100,500) in our grid search. The optimal model with RBF kernel is using gamma=0.1. As gamma controls the wideness of the radial basis function and a lower gamma indicates a wider bumps, for gamma=0.1, the model is prone to underfit, because this means each data point is less influencial to the model and the decision boundary is simpler, it may miss some patterns and make more mistakes.\n",
    "\n",
    " **Polynomial**\n",
    "\n",
    " We tried polynomial kernel with a default degree of 3. Based on the grid search, this kernel works best. Since our dataset is not linearly separable, polynomial kernal helps the dataset embedded in a higher dimensional space, in which the points may be easier to classify with a linear classifier. Moreover, it obtained relatively low values in RMSE, MSE and MAE, with a relatively better R^2 results in both training and validation set.      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    }
   ],
   "source": [
    "# grid search for the best parameters\n",
    "param_grid = {'kernel': ['linear', 'poly', 'rbf'],\n",
    "              'gamma':[0.1,1,10,100,500], \n",
    "              'C': [1, 10, 100, 1000, 2000]}\n",
    "\n",
    "\n",
    "# set the grid search\n",
    "grid_search = GridSearchCV(SVR(), param_grid, scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# fit the grid search\n",
    "grid = grid_search.fit(features_training, label_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.407096e+06</td>\n",
       "      <td>60347.588629</td>\n",
       "      <td>{'C': 2000, 'gamma': 1, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.427492e+06</td>\n",
       "      <td>49203.445421</td>\n",
       "      <td>{'C': 1, 'gamma': 10, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.427492e+06</td>\n",
       "      <td>49203.391071</td>\n",
       "      <td>{'C': 1000, 'gamma': 1, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.438542e+06</td>\n",
       "      <td>69231.677684</td>\n",
       "      <td>{'C': 10, 'gamma': 10, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>-1.577989e+06</td>\n",
       "      <td>36677.906539</td>\n",
       "      <td>{'C': 100, 'gamma': 1, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6</td>\n",
       "      <td>-1.656418e+06</td>\n",
       "      <td>49297.560538</td>\n",
       "      <td>{'C': 100, 'gamma': 10, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>-1.710730e+06</td>\n",
       "      <td>99603.468544</td>\n",
       "      <td>{'C': 10, 'gamma': 1, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>8</td>\n",
       "      <td>-1.839473e+06</td>\n",
       "      <td>129664.499749</td>\n",
       "      <td>{'C': 2000, 'gamma': 10, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>8</td>\n",
       "      <td>-1.839473e+06</td>\n",
       "      <td>129664.499749</td>\n",
       "      <td>{'C': 2000, 'gamma': 500, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>8</td>\n",
       "      <td>-1.839473e+06</td>\n",
       "      <td>129664.499749</td>\n",
       "      <td>{'C': 2000, 'gamma': 100, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>8</td>\n",
       "      <td>-1.839473e+06</td>\n",
       "      <td>129664.499749</td>\n",
       "      <td>{'C': 2000, 'gamma': 0.1, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>8</td>\n",
       "      <td>-1.839473e+06</td>\n",
       "      <td>129664.499749</td>\n",
       "      <td>{'C': 2000, 'gamma': 1, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>13</td>\n",
       "      <td>-1.970714e+06</td>\n",
       "      <td>136025.825601</td>\n",
       "      <td>{'C': 2000, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>14</td>\n",
       "      <td>-2.020582e+06</td>\n",
       "      <td>139693.536089</td>\n",
       "      <td>{'C': 1000, 'gamma': 10, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>14</td>\n",
       "      <td>-2.020582e+06</td>\n",
       "      <td>139693.536089</td>\n",
       "      <td>{'C': 1000, 'gamma': 1, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>14</td>\n",
       "      <td>-2.020582e+06</td>\n",
       "      <td>139693.536089</td>\n",
       "      <td>{'C': 1000, 'gamma': 500, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>14</td>\n",
       "      <td>-2.020582e+06</td>\n",
       "      <td>139693.536089</td>\n",
       "      <td>{'C': 1000, 'gamma': 100, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>14</td>\n",
       "      <td>-2.020582e+06</td>\n",
       "      <td>139693.536089</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.1, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>19</td>\n",
       "      <td>-2.189837e+06</td>\n",
       "      <td>149379.579932</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>-2.189837e+06</td>\n",
       "      <td>149379.579932</td>\n",
       "      <td>{'C': 1, 'gamma': 1, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>21</td>\n",
       "      <td>-2.329245e+06</td>\n",
       "      <td>360392.249229</td>\n",
       "      <td>{'C': 1000, 'gamma': 10, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22</td>\n",
       "      <td>-2.329305e+06</td>\n",
       "      <td>360398.698148</td>\n",
       "      <td>{'C': 1, 'gamma': 100, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>23</td>\n",
       "      <td>-2.363040e+06</td>\n",
       "      <td>154557.781111</td>\n",
       "      <td>{'C': 2000, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>24</td>\n",
       "      <td>-2.408061e+06</td>\n",
       "      <td>158189.247272</td>\n",
       "      <td>{'C': 2000, 'gamma': 1, 'kernel': 'rbf'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>25</td>\n",
       "      <td>-2.416966e+06</td>\n",
       "      <td>160989.968575</td>\n",
       "      <td>{'C': 100, 'gamma': 100, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>25</td>\n",
       "      <td>-2.416966e+06</td>\n",
       "      <td>160989.968575</td>\n",
       "      <td>{'C': 100, 'gamma': 1, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>25</td>\n",
       "      <td>-2.416966e+06</td>\n",
       "      <td>160989.968575</td>\n",
       "      <td>{'C': 100, 'gamma': 0.1, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>25</td>\n",
       "      <td>-2.416966e+06</td>\n",
       "      <td>160989.968575</td>\n",
       "      <td>{'C': 100, 'gamma': 10, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>25</td>\n",
       "      <td>-2.416966e+06</td>\n",
       "      <td>160989.968575</td>\n",
       "      <td>{'C': 100, 'gamma': 500, 'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>30</td>\n",
       "      <td>-2.425205e+06</td>\n",
       "      <td>162068.033019</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank_test_score  mean_test_score  std_test_score  \\\n",
       "64                1    -1.407096e+06    60347.588629   \n",
       "7                 2    -1.427492e+06    49203.445421   \n",
       "49                3    -1.427492e+06    49203.391071   \n",
       "22                4    -1.438542e+06    69231.677684   \n",
       "34                5    -1.577989e+06    36677.906539   \n",
       "37                6    -1.656418e+06    49297.560538   \n",
       "19                7    -1.710730e+06    99603.468544   \n",
       "66                8    -1.839473e+06   129664.499749   \n",
       "72                8    -1.839473e+06   129664.499749   \n",
       "69                8    -1.839473e+06   129664.499749   \n",
       "60                8    -1.839473e+06   129664.499749   \n",
       "63                8    -1.839473e+06   129664.499749   \n",
       "61               13    -1.970714e+06   136025.825601   \n",
       "51               14    -2.020582e+06   139693.536089   \n",
       "48               14    -2.020582e+06   139693.536089   \n",
       "57               14    -2.020582e+06   139693.536089   \n",
       "54               14    -2.020582e+06   139693.536089   \n",
       "45               14    -2.020582e+06   139693.536089   \n",
       "46               19    -2.189837e+06   149379.579932   \n",
       "4                20    -2.189837e+06   149379.579932   \n",
       "52               21    -2.329245e+06   360392.249229   \n",
       "10               22    -2.329305e+06   360398.698148   \n",
       "62               23    -2.363040e+06   154557.781111   \n",
       "65               24    -2.408061e+06   158189.247272   \n",
       "39               25    -2.416966e+06   160989.968575   \n",
       "33               25    -2.416966e+06   160989.968575   \n",
       "30               25    -2.416966e+06   160989.968575   \n",
       "36               25    -2.416966e+06   160989.968575   \n",
       "42               25    -2.416966e+06   160989.968575   \n",
       "47               30    -2.425205e+06   162068.033019   \n",
       "\n",
       "                                           params  \n",
       "64      {'C': 2000, 'gamma': 1, 'kernel': 'poly'}  \n",
       "7         {'C': 1, 'gamma': 10, 'kernel': 'poly'}  \n",
       "49      {'C': 1000, 'gamma': 1, 'kernel': 'poly'}  \n",
       "22       {'C': 10, 'gamma': 10, 'kernel': 'poly'}  \n",
       "34       {'C': 100, 'gamma': 1, 'kernel': 'poly'}  \n",
       "37      {'C': 100, 'gamma': 10, 'kernel': 'poly'}  \n",
       "19        {'C': 10, 'gamma': 1, 'kernel': 'poly'}  \n",
       "66   {'C': 2000, 'gamma': 10, 'kernel': 'linear'}  \n",
       "72  {'C': 2000, 'gamma': 500, 'kernel': 'linear'}  \n",
       "69  {'C': 2000, 'gamma': 100, 'kernel': 'linear'}  \n",
       "60  {'C': 2000, 'gamma': 0.1, 'kernel': 'linear'}  \n",
       "63    {'C': 2000, 'gamma': 1, 'kernel': 'linear'}  \n",
       "61    {'C': 2000, 'gamma': 0.1, 'kernel': 'poly'}  \n",
       "51   {'C': 1000, 'gamma': 10, 'kernel': 'linear'}  \n",
       "48    {'C': 1000, 'gamma': 1, 'kernel': 'linear'}  \n",
       "57  {'C': 1000, 'gamma': 500, 'kernel': 'linear'}  \n",
       "54  {'C': 1000, 'gamma': 100, 'kernel': 'linear'}  \n",
       "45  {'C': 1000, 'gamma': 0.1, 'kernel': 'linear'}  \n",
       "46    {'C': 1000, 'gamma': 0.1, 'kernel': 'poly'}  \n",
       "4          {'C': 1, 'gamma': 1, 'kernel': 'poly'}  \n",
       "52     {'C': 1000, 'gamma': 10, 'kernel': 'poly'}  \n",
       "10       {'C': 1, 'gamma': 100, 'kernel': 'poly'}  \n",
       "62     {'C': 2000, 'gamma': 0.1, 'kernel': 'rbf'}  \n",
       "65       {'C': 2000, 'gamma': 1, 'kernel': 'rbf'}  \n",
       "39   {'C': 100, 'gamma': 100, 'kernel': 'linear'}  \n",
       "33     {'C': 100, 'gamma': 1, 'kernel': 'linear'}  \n",
       "30   {'C': 100, 'gamma': 0.1, 'kernel': 'linear'}  \n",
       "36    {'C': 100, 'gamma': 10, 'kernel': 'linear'}  \n",
       "42   {'C': 100, 'gamma': 500, 'kernel': 'linear'}  \n",
       "47     {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the grid search results, sorted by test score from the best\n",
    "grid_result = pd.DataFrame(grid.cv_results_, \n",
    "                           columns=['rank_test_score','mean_test_score', 'std_test_score', 'params']).sort_values(by=['rank_test_score'], \n",
    "                           ascending=True)\n",
    "\n",
    "# set the display to show all the columns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# check the top 10 results\n",
    "grid_result.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Find predictions using the validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii.Use the score() function to display the training/validation coefficient of determination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii.Decide which metrics to use in analysis (e.g. absolute error, RMSE, MSE, recall, precision, Fβ score, etc.); calculate them using the validation set and display them. Explain how these metrics will help you decide which model is best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model is a regression model, we will exclude classification metrics and focus on mainly three metrics in our analysis, namely, mean absolute error (MAE), mean square error (MSE) and root mean square error(RMSE). \n",
    "\n",
    "**MAE** measures the average absolute difference between the predicted and actual salaries, so we are expecting a model with a lower MAE. Since it does not square the error, it is less sensitive to outliers compared to MSE and RMSE. \n",
    "\n",
    "**MSE** measures the average squared difference between the predicted and actual salaries. We are expecting a model with a lower MSE because it means the predicated salaries are closer to the actual earning. MSE gives higher weight to larger errors, which makes it sensitive to outliers.\n",
    "\n",
    "Same with **RMSE** which is the square root of MSE, it measures the average error in the same units as the label which makes it easy to interpret. The lower the RMSE, the closer the prediction to the actually earning.\n",
    "\n",
    "Additionally, we could also consider **R^2** when comparing models, since R^2 provides a measure of how well the model fits our data and help assess the proportion of variability in the label that can be explained by the model. It ranges from 0 to 1, a higher R^2 indicates a better fit of the model to the data. But we need to be aware that a model with a high R^2 value may not necessarily be a good predictor, as it may overfit the data resulting poor prediction on new data.\n",
    "\n",
    "In a nutshell, since our goal here is to minimize the prediction errors with a disparity of earnings, the metrics like MSE, RMSE may be more appropriate in our case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'C': 2000, 'gamma': 1, 'kernel': 'poly'}\n",
      "        Actual     Predicted  R^2_validation  R^2_training     MAE  \\\n",
      "11    667500.0  7.011217e+05        0.674138      0.657556  848729   \n",
      "558   630000.0  6.756916e+05        0.674138      0.657556  848729   \n",
      "337   832500.0  1.777296e+06        0.674138      0.657556  848729   \n",
      "244  2000000.0  8.024228e+05        0.674138      0.657556  848729   \n",
      "634  4000000.0  3.127388e+06        0.674138      0.657556  848729   \n",
      "..         ...           ...             ...           ...     ...   \n",
      "784   750000.0  8.780772e+05        0.674138      0.657556  848729   \n",
      "309   632500.0  7.385167e+05        0.674138      0.657556  848729   \n",
      "59   5250000.0  2.369878e+06        0.674138      0.657556  848729   \n",
      "161  3500000.0  1.233833e+06        0.674138      0.657556  848729   \n",
      "435  4500000.0  3.784327e+06        0.674138      0.657556  848729   \n",
      "\n",
      "              MSE     RMSE  \n",
      "11   1.615649e+12  1271081  \n",
      "558  1.615649e+12  1271081  \n",
      "337  1.615649e+12  1271081  \n",
      "244  1.615649e+12  1271081  \n",
      "634  1.615649e+12  1271081  \n",
      "..            ...      ...  \n",
      "784  1.615649e+12  1271081  \n",
      "309  1.615649e+12  1271081  \n",
      "59   1.615649e+12  1271081  \n",
      "161  1.615649e+12  1271081  \n",
      "435  1.615649e+12  1271081  \n",
      "\n",
      "[78 rows x 7 columns]\n",
      "--------------------------------------------------------------------------\n",
      "Parameters: {'C': 1, 'gamma': 10, 'kernel': 'poly'}\n",
      "        Actual     Predicted  R^2_validation  R^2_training     MAE  \\\n",
      "11    667500.0  7.009422e+05        0.657049      0.626675  876660   \n",
      "558   630000.0  6.847477e+05        0.657049      0.626675  876660   \n",
      "337   832500.0  2.045026e+06        0.657049      0.626675  876660   \n",
      "244  2000000.0  7.874335e+05        0.657049      0.626675  876660   \n",
      "634  4000000.0  3.035438e+06        0.657049      0.626675  876660   \n",
      "..         ...           ...             ...           ...     ...   \n",
      "784   750000.0  8.718185e+05        0.657049      0.626675  876660   \n",
      "309   632500.0  7.333323e+05        0.657049      0.626675  876660   \n",
      "59   5250000.0  2.158919e+06        0.657049      0.626675  876660   \n",
      "161  3500000.0  1.141313e+06        0.657049      0.626675  876660   \n",
      "435  4500000.0  3.957317e+06        0.657049      0.626675  876660   \n",
      "\n",
      "              MSE     RMSE  \n",
      "11   1.700377e+12  1303985  \n",
      "558  1.700377e+12  1303985  \n",
      "337  1.700377e+12  1303985  \n",
      "244  1.700377e+12  1303985  \n",
      "634  1.700377e+12  1303985  \n",
      "..            ...      ...  \n",
      "784  1.700377e+12  1303985  \n",
      "309  1.700377e+12  1303985  \n",
      "59   1.700377e+12  1303985  \n",
      "161  1.700377e+12  1303985  \n",
      "435  1.700377e+12  1303985  \n",
      "\n",
      "[78 rows x 7 columns]\n",
      "--------------------------------------------------------------------------\n",
      "Parameters: {'C': 1000, 'gamma': 1, 'kernel': 'poly'}\n",
      "        Actual     Predicted  R^2_validation  R^2_training     MAE  \\\n",
      "11    667500.0  7.009423e+05        0.657049      0.626675  876660   \n",
      "558   630000.0  6.847476e+05        0.657049      0.626675  876660   \n",
      "337   832500.0  2.045024e+06        0.657049      0.626675  876660   \n",
      "244  2000000.0  7.874336e+05        0.657049      0.626675  876660   \n",
      "634  4000000.0  3.035435e+06        0.657049      0.626675  876660   \n",
      "..         ...           ...             ...           ...     ...   \n",
      "784   750000.0  8.718185e+05        0.657049      0.626675  876660   \n",
      "309   632500.0  7.333325e+05        0.657049      0.626675  876660   \n",
      "59   5250000.0  2.158920e+06        0.657049      0.626675  876660   \n",
      "161  3500000.0  1.141314e+06        0.657049      0.626675  876660   \n",
      "435  4500000.0  3.957312e+06        0.657049      0.626675  876660   \n",
      "\n",
      "              MSE     RMSE  \n",
      "11   1.700377e+12  1303985  \n",
      "558  1.700377e+12  1303985  \n",
      "337  1.700377e+12  1303985  \n",
      "244  1.700377e+12  1303985  \n",
      "634  1.700377e+12  1303985  \n",
      "..            ...      ...  \n",
      "784  1.700377e+12  1303985  \n",
      "309  1.700377e+12  1303985  \n",
      "59   1.700377e+12  1303985  \n",
      "161  1.700377e+12  1303985  \n",
      "435  1.700377e+12  1303985  \n",
      "\n",
      "[78 rows x 7 columns]\n",
      "--------------------------------------------------------------------------\n",
      "Parameters: {'C': 10, 'gamma': 10, 'kernel': 'poly'}\n",
      "        Actual     Predicted  R^2_validation  R^2_training     MAE  \\\n",
      "11    667500.0  6.944914e+05        0.747287      0.739822  753749   \n",
      "558   630000.0  6.701764e+05        0.747287      0.739822  753749   \n",
      "337   832500.0  1.722246e+06        0.747287      0.739822  753749   \n",
      "244  2000000.0  8.317825e+05        0.747287      0.739822  753749   \n",
      "634  4000000.0  3.179417e+06        0.747287      0.739822  753749   \n",
      "..         ...           ...             ...           ...     ...   \n",
      "784   750000.0  9.374576e+05        0.747287      0.739822  753749   \n",
      "309   632500.0  7.007807e+05        0.747287      0.739822  753749   \n",
      "59   5250000.0  2.813504e+06        0.747287      0.739822  753749   \n",
      "161  3500000.0  1.392539e+06        0.747287      0.739822  753749   \n",
      "435  4500000.0  3.540376e+06        0.747287      0.739822  753749   \n",
      "\n",
      "              MSE     RMSE  \n",
      "11   1.252971e+12  1119361  \n",
      "558  1.252971e+12  1119361  \n",
      "337  1.252971e+12  1119361  \n",
      "244  1.252971e+12  1119361  \n",
      "634  1.252971e+12  1119361  \n",
      "..            ...      ...  \n",
      "784  1.252971e+12  1119361  \n",
      "309  1.252971e+12  1119361  \n",
      "59   1.252971e+12  1119361  \n",
      "161  1.252971e+12  1119361  \n",
      "435  1.252971e+12  1119361  \n",
      "\n",
      "[78 rows x 7 columns]\n",
      "--------------------------------------------------------------------------\n",
      "Parameters: {'C': 100, 'gamma': 1, 'kernel': 'poly'}\n",
      "        Actual     Predicted  R^2_validation  R^2_training     MAE  \\\n",
      "11    667500.0  6.958024e+05        0.554365      0.476053  984954   \n",
      "558   630000.0  6.945242e+05        0.554365      0.476053  984954   \n",
      "337   832500.0  3.222199e+06        0.554365      0.476053  984954   \n",
      "244  2000000.0  7.402718e+05        0.554365      0.476053  984954   \n",
      "634  4000000.0  2.979922e+06        0.554365      0.476053  984954   \n",
      "..         ...           ...             ...           ...     ...   \n",
      "784   750000.0  7.936867e+05        0.554365      0.476053  984954   \n",
      "309   632500.0  7.103469e+05        0.554365      0.476053  984954   \n",
      "59   5250000.0  1.575275e+06        0.554365      0.476053  984954   \n",
      "161  3500000.0  9.659079e+05        0.554365      0.476053  984954   \n",
      "435  4500000.0  4.205057e+06        0.554365      0.476053  984954   \n",
      "\n",
      "              MSE     RMSE  \n",
      "11   2.209493e+12  1486436  \n",
      "558  2.209493e+12  1486436  \n",
      "337  2.209493e+12  1486436  \n",
      "244  2.209493e+12  1486436  \n",
      "634  2.209493e+12  1486436  \n",
      "..            ...      ...  \n",
      "784  2.209493e+12  1486436  \n",
      "309  2.209493e+12  1486436  \n",
      "59   2.209493e+12  1486436  \n",
      "161  2.209493e+12  1486436  \n",
      "435  2.209493e+12  1486436  \n",
      "\n",
      "[78 rows x 7 columns]\n",
      "--------------------------------------------------------------------------\n",
      "Parameters: {'C': 100, 'gamma': 10, 'kernel': 'poly'}\n",
      "        Actual     Predicted  R^2_validation  R^2_training     MAE  \\\n",
      "11    667500.0  6.746749e+05        0.692325      0.870332  827557   \n",
      "558   630000.0  6.539236e+05        0.692325      0.870332  827557   \n",
      "337   832500.0  2.473084e+06        0.692325      0.870332  827557   \n",
      "244  2000000.0  7.948991e+05        0.692325      0.870332  827557   \n",
      "634  4000000.0  2.414173e+06        0.692325      0.870332  827557   \n",
      "..         ...           ...             ...           ...     ...   \n",
      "784   750000.0  1.139745e+06        0.692325      0.870332  827557   \n",
      "309   632500.0  6.358795e+05        0.692325      0.870332  827557   \n",
      "59   5250000.0  2.819294e+06        0.692325      0.870332  827557   \n",
      "161  3500000.0  1.352147e+06        0.692325      0.870332  827557   \n",
      "435  4500000.0  2.696459e+06        0.692325      0.870332  827557   \n",
      "\n",
      "              MSE     RMSE  \n",
      "11   1.525474e+12  1235100  \n",
      "558  1.525474e+12  1235100  \n",
      "337  1.525474e+12  1235100  \n",
      "244  1.525474e+12  1235100  \n",
      "634  1.525474e+12  1235100  \n",
      "..            ...      ...  \n",
      "784  1.525474e+12  1235100  \n",
      "309  1.525474e+12  1235100  \n",
      "59   1.525474e+12  1235100  \n",
      "161  1.525474e+12  1235100  \n",
      "435  1.525474e+12  1235100  \n",
      "\n",
      "[78 rows x 7 columns]\n",
      "--------------------------------------------------------------------------\n",
      "Parameters: {'C': 10, 'gamma': 1, 'kernel': 'poly'}\n",
      "        Actual     Predicted  R^2_validation  R^2_training      MAE  \\\n",
      "11    667500.0  7.459542e+05        0.410004      0.355741  1125513   \n",
      "558   630000.0  7.394895e+05        0.410004      0.355741  1125513   \n",
      "337   832500.0  3.202481e+06        0.410004      0.355741  1125513   \n",
      "244  2000000.0  7.701968e+05        0.410004      0.355741  1125513   \n",
      "634  4000000.0  3.021111e+06        0.410004      0.355741  1125513   \n",
      "..         ...           ...             ...           ...      ...   \n",
      "784   750000.0  7.949311e+05        0.410004      0.355741  1125513   \n",
      "309   632500.0  7.625367e+05        0.410004      0.355741  1125513   \n",
      "59   5250000.0  1.434590e+06        0.410004      0.355741  1125513   \n",
      "161  3500000.0  9.589630e+05        0.410004      0.355741  1125513   \n",
      "435  4500000.0  4.033618e+06        0.410004      0.355741  1125513   \n",
      "\n",
      "              MSE     RMSE  \n",
      "11   2.925247e+12  1710335  \n",
      "558  2.925247e+12  1710335  \n",
      "337  2.925247e+12  1710335  \n",
      "244  2.925247e+12  1710335  \n",
      "634  2.925247e+12  1710335  \n",
      "..            ...      ...  \n",
      "784  2.925247e+12  1710335  \n",
      "309  2.925247e+12  1710335  \n",
      "59   2.925247e+12  1710335  \n",
      "161  2.925247e+12  1710335  \n",
      "435  2.925247e+12  1710335  \n",
      "\n",
      "[78 rows x 7 columns]\n",
      "--------------------------------------------------------------------------\n",
      "Parameters: {'C': 2000, 'gamma': 10, 'kernel': 'linear'}\n",
      "        Actual     Predicted  R^2_validation  R^2_training      MAE  \\\n",
      "11    667500.0  7.754755e+05        0.287596      0.272891  1308418   \n",
      "558   630000.0  5.883929e+05        0.287596      0.272891  1308418   \n",
      "337   832500.0  2.635815e+06        0.287596      0.272891  1308418   \n",
      "244  2000000.0  9.602888e+05        0.287596      0.272891  1308418   \n",
      "634  4000000.0  2.606311e+06        0.287596      0.272891  1308418   \n",
      "..         ...           ...             ...           ...      ...   \n",
      "784   750000.0  1.074779e+06        0.287596      0.272891  1308418   \n",
      "309   632500.0  9.169545e+05        0.287596      0.272891  1308418   \n",
      "59   5250000.0  1.868693e+06        0.287596      0.272891  1308418   \n",
      "161  3500000.0  1.419313e+06        0.287596      0.272891  1308418   \n",
      "435  4500000.0  2.896289e+06        0.287596      0.272891  1308418   \n",
      "\n",
      "              MSE     RMSE  \n",
      "11   3.532155e+12  1879402  \n",
      "558  3.532155e+12  1879402  \n",
      "337  3.532155e+12  1879402  \n",
      "244  3.532155e+12  1879402  \n",
      "634  3.532155e+12  1879402  \n",
      "..            ...      ...  \n",
      "784  3.532155e+12  1879402  \n",
      "309  3.532155e+12  1879402  \n",
      "59   3.532155e+12  1879402  \n",
      "161  3.532155e+12  1879402  \n",
      "435  3.532155e+12  1879402  \n",
      "\n",
      "[78 rows x 7 columns]\n",
      "--------------------------------------------------------------------------\n",
      "Parameters: {'C': 2000, 'gamma': 500, 'kernel': 'linear'}\n",
      "        Actual     Predicted  R^2_validation  R^2_training      MAE  \\\n",
      "11    667500.0  7.754755e+05        0.287596      0.272891  1308418   \n",
      "558   630000.0  5.883929e+05        0.287596      0.272891  1308418   \n",
      "337   832500.0  2.635815e+06        0.287596      0.272891  1308418   \n",
      "244  2000000.0  9.602888e+05        0.287596      0.272891  1308418   \n",
      "634  4000000.0  2.606311e+06        0.287596      0.272891  1308418   \n",
      "..         ...           ...             ...           ...      ...   \n",
      "784   750000.0  1.074779e+06        0.287596      0.272891  1308418   \n",
      "309   632500.0  9.169545e+05        0.287596      0.272891  1308418   \n",
      "59   5250000.0  1.868693e+06        0.287596      0.272891  1308418   \n",
      "161  3500000.0  1.419313e+06        0.287596      0.272891  1308418   \n",
      "435  4500000.0  2.896289e+06        0.287596      0.272891  1308418   \n",
      "\n",
      "              MSE     RMSE  \n",
      "11   3.532155e+12  1879402  \n",
      "558  3.532155e+12  1879402  \n",
      "337  3.532155e+12  1879402  \n",
      "244  3.532155e+12  1879402  \n",
      "634  3.532155e+12  1879402  \n",
      "..            ...      ...  \n",
      "784  3.532155e+12  1879402  \n",
      "309  3.532155e+12  1879402  \n",
      "59   3.532155e+12  1879402  \n",
      "161  3.532155e+12  1879402  \n",
      "435  3.532155e+12  1879402  \n",
      "\n",
      "[78 rows x 7 columns]\n",
      "--------------------------------------------------------------------------\n",
      "Parameters: {'C': 2000, 'gamma': 100, 'kernel': 'linear'}\n",
      "        Actual     Predicted  R^2_validation  R^2_training      MAE  \\\n",
      "11    667500.0  7.754755e+05        0.287596      0.272891  1308418   \n",
      "558   630000.0  5.883929e+05        0.287596      0.272891  1308418   \n",
      "337   832500.0  2.635815e+06        0.287596      0.272891  1308418   \n",
      "244  2000000.0  9.602888e+05        0.287596      0.272891  1308418   \n",
      "634  4000000.0  2.606311e+06        0.287596      0.272891  1308418   \n",
      "..         ...           ...             ...           ...      ...   \n",
      "784   750000.0  1.074779e+06        0.287596      0.272891  1308418   \n",
      "309   632500.0  9.169545e+05        0.287596      0.272891  1308418   \n",
      "59   5250000.0  1.868693e+06        0.287596      0.272891  1308418   \n",
      "161  3500000.0  1.419313e+06        0.287596      0.272891  1308418   \n",
      "435  4500000.0  2.896289e+06        0.287596      0.272891  1308418   \n",
      "\n",
      "              MSE     RMSE  \n",
      "11   3.532155e+12  1879402  \n",
      "558  3.532155e+12  1879402  \n",
      "337  3.532155e+12  1879402  \n",
      "244  3.532155e+12  1879402  \n",
      "634  3.532155e+12  1879402  \n",
      "..            ...      ...  \n",
      "784  3.532155e+12  1879402  \n",
      "309  3.532155e+12  1879402  \n",
      "59   3.532155e+12  1879402  \n",
      "161  3.532155e+12  1879402  \n",
      "435  3.532155e+12  1879402  \n",
      "\n",
      "[78 rows x 7 columns]\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# keep the parameters for the top 20 results, so we can test three kernels\n",
    "params = grid_result['params'].head(10)\n",
    "\n",
    "# create a dictionary to store the best parameters\n",
    "best_para ={}\n",
    "\n",
    "# create a variable to store the minimum rmse\n",
    "min_rmse=10000000\n",
    "\n",
    "for i in params:\n",
    "    svr = SVR(C=i['C'], gamma=i['gamma'], kernel=i['kernel'])\n",
    "    model = svr.fit(features_training, label_training)\n",
    "\n",
    "    # make predictions using validation set\n",
    "    pred = model.predict(features_validation)\n",
    "\n",
    "    # return the training/validation coefficient of determination (R^2)\n",
    "    score_training = model.score(features_training, label_training)\n",
    "    score_validation= model.score(features_validation, label_validation)\n",
    "\n",
    "    # return the mean absolute error\n",
    "    mae = mean_absolute_error(label_validation, pred).astype(int)\n",
    "\n",
    "    # return the mean squared error\n",
    "    mse = mean_squared_error(label_validation, pred)\n",
    "    \n",
    "    # return the root mean squared error\n",
    "    rmse = mean_squared_error(label_validation, pred, squared=False).astype(int) \n",
    "    \n",
    "    # create a dataframe to compare different models\n",
    "    compare = pd.DataFrame({'Actual': label_validation, \n",
    "                            'Predicted': pred, \n",
    "                            'R^2_validation': score_validation,\n",
    "                            'R^2_training': score_training, \n",
    "                            'MAE':mae, \n",
    "                            'MSE':mse,\n",
    "                            'RMSE':rmse})\n",
    "     # check the parameter with the lowest rmse\n",
    "    if rmse<min_rmse:\n",
    "        best_para = i\n",
    "        min_rmse=rmse\n",
    "\n",
    "    print(\"Parameters:\", i)\n",
    "    print(compare)\n",
    "    print('--------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models and justify a final choice of hyperparameters based on the above quantitative analysis and the use case. Comment on overfitting, underfitting and any qualitative reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from the above ten models: {'C': 10, 'gamma': 10, 'kernel': 'poly'} with RMSE: 1119361\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters from the above ten models:\", best_para, \"with RMSE:\", min_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Model from the above ten models:**\n",
    "\n",
    "Paramters: {'C': 10, 'gamma': 10, 'kernel': 'poly'} \n",
    "\n",
    "MSE = 1252971000000\n",
    "\n",
    "RMSE = 1119361\n",
    "\n",
    "MAE = 753749\n",
    "\n",
    "R^2 (Validation) = 0.747287      \n",
    "\n",
    "R^2 (Training) = 0.739822  \n",
    "\n",
    "Based on our chosen evaluation metrics, we prioritized the model with the lowest RMSE and MSE values for further analysis. \n",
    "\n",
    "The RMSE was calculated be \\\\$1119361, representing the average squared root difference between the predicted and actual salaries, and the MAE to be \\\\$753749, representing the average absolute difference. We observed that the model with the lowest RMSE outperforms the other models. The MSE, RMSE, and MAE values are relatively lower than those of the other models, indicating that the model's predictions are closer to the actual salaries and it can provide more accurate salary estimates.\n",
    "\n",
    "Furthermore, we observed the R-squared value for both the training set and validation set is better than the rest. The R^2 value was calculated to be 0.74 for the training set, indicating that 74% of the variance in the salaries can be explained by the model. Similarly, the R^2 value was also found to be 0.74 for the validation set, indicating that the model captures 74% of the variability in salaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2000, 'gamma': 1, 'kernel': 'poly'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that the grid search obtained a different set of best parameters. This could be due to the fact that we did not perform cross-validation when selecting the best parameters from the ten models evaluated in the grid search. As more data is fed into the model, the optimal parameters may change. While during the grid search, we utilized five-fold cross-validation, meaning splitting the training data into five subsets for model evaluation. Therefore, we  trust the obtained result and will use the parameter set from grid search. But the logic behind selecting the best parameters was based on a combination of quantitative and qualitative analysis, as explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the predicted and acutal salaries to exam where to improve next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(C=2000, gamma=1, kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(C=2000, gamma=1, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR(C=2000, gamma=1, kernel='poly')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a model with the best parameters\n",
    "best_svr=SVR(C=2000, gamma=1, kernel='poly')\n",
    "best_svr.fit(features_training, label_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the prediction\n",
    "pred=best_svr.predict(features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>667500</td>\n",
       "      <td>701121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>630000</td>\n",
       "      <td>675691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>832500</td>\n",
       "      <td>1777295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>2000000</td>\n",
       "      <td>802422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>4000000</td>\n",
       "      <td>3127388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>750000</td>\n",
       "      <td>878077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>632500</td>\n",
       "      <td>738516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5250000</td>\n",
       "      <td>2369878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>3500000</td>\n",
       "      <td>1233832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>4500000</td>\n",
       "      <td>3784326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual  Predicted\n",
       "11    667500     701121\n",
       "558   630000     675691\n",
       "337   832500    1777295\n",
       "244  2000000     802422\n",
       "634  4000000    3127388\n",
       "..       ...        ...\n",
       "784   750000     878077\n",
       "309   632500     738516\n",
       "59   5250000    2369878\n",
       "161  3500000    1233832\n",
       "435  4500000    3784326\n",
       "\n",
       "[78 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe to compare the actual and predicted values\n",
    "acts=pd.DataFrame({'Actual':label_validation, 'Predicted':pred}).astype(int)\n",
    "acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the actual and predicted values\n",
    "acts.plot(kind='bar',figsize=(10,6))\n",
    "# set the title\n",
    "plt.title('Actual vs Predicted')\n",
    "# set the x_axis ticks to empty\n",
    "plt.xticks([])\n",
    "# set the grid\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.1', color='green')\n",
    "plt.grid(which='minor', linestyle='-', linewidth='0.1', color='black')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>7.034003e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOI/GP</th>\n",
       "      <td>6.400055e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOI%</th>\n",
       "      <td>4.400915e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOL</th>\n",
       "      <td>1.933317e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GS/G</th>\n",
       "      <td>1.680606e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S.Snap</th>\n",
       "      <td>1.619414e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shifts</th>\n",
       "      <td>1.592029e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SF</th>\n",
       "      <td>1.330678e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DPS</th>\n",
       "      <td>1.162280e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iSCF</th>\n",
       "      <td>1.144310e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Importance\n",
       "Age     7.034003e+11\n",
       "TOI/GP  6.400055e+11\n",
       "TOI%    4.400915e+11\n",
       "FOL     1.933317e+11\n",
       "GS/G    1.680606e+11\n",
       "S.Snap  1.619414e+11\n",
       "Shifts  1.592029e+11\n",
       "SF      1.330678e+11\n",
       "DPS     1.162280e+11\n",
       "iSCF    1.144310e+11"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform permutation importance\n",
    "results = permutation_importance(best_svr, features_training, label_training, scoring='neg_mean_squared_error')\n",
    "# get importance\n",
    "importance = results.importances_mean\n",
    "# summarize feature importance\n",
    "importance = pd.DataFrame(results.importances_mean, index=features_training.columns,columns=['Importance'])\n",
    "# sort features by importance\n",
    "importance = importance.sort_values(by='Importance', ascending=False)\n",
    "importance.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the model in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_svm.pkl', 'wb') as file:\n",
    "    file.write(pickle.dumps(best_svr))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
